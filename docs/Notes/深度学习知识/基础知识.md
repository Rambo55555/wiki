BN和LN的区别：**https://blog.csdn.net/HUSTHY/article/details/106665809**

交叉熵

## KL散度

**信息量**：小概率事件不确定性程度很高，包含的信息很大。反之，大概率事件很有可能发生，包含的信息更小。为了度量一个事件的信息量，直觉想法是直接用概率的倒数$\frac{1}{p(x)}$表示。但是我们又想要能够衡量多件独立事件的信息和，因此通过添加一个对数，使得多件独立事件的信息量能够相加。信息量$I(x)$表示为：

$$I(x)=log_{2}(\frac{1}{p(x)})$$

例如抛硬币事件，正面head和反面tail概率都为0.5，那么它们的信息量为：

$$I(head)=I(tail)=log_{2}(\frac{1}{0.5})=1$$

**香农熵**：信息量是衡量的事件的信息，在现实世界中一个事件有多种结果，所有可能结果和发生概率组成了概率分布。为了衡量概率分布的信息量，通过计算香农熵得到：

$$H(p)=\sum{p_iI_i}=\sum{p_ilog_2{\frac{1}{p_i}}}=-\sum{p_ilog_2{p_i}}$$

例如抛硬币事件，正面head和反面tail概率都为0.5，那么在该概率分布下，该事件的香农熵为：

$$H(p)=p_{head}\times{I(head)}+p_{tail}\times{I(tail)}=0.5\times1+0.5\times1=1$$

当分布越均匀的时候，熵越大。当分布越不均匀，熵越小

**交叉熵**：现实世界中事件发生有一个真实概率，比如说两面均匀的硬币，$p(head)=0.5$ $p(tail)=0.5$。我们可以通过多次抛硬币得到一个估计概率，假设估计概率为： $q(head)=0.8$ $q(tail)=0.2$。那么估计概率分布对于真实概率分布的信息量的估计叫做交叉熵$H(p,q)$:

$$H(p,q)=\sum{p_iI(q_i)}=-\sum{p_ilog_2(q_i)}$$

例如抛硬币例子，$H(p,q)=p(head)\times{log_2(\frac{1}{q(head)})}+p(tail)\times{log_2(\frac{1}{q(tail)})}=1.32$

交叉熵越小，估计的概率分布越接近真实概率分布

**KL散度**：衡量两个概率分布的区别，通过交叉熵减去熵来计算得到

$$D(p||q)=H(p,q)-H(p)=\sum{p_iI(q_i)}-\sum{p_iI(p_i)}=\sum{p_ilog_2(\frac{p_i}{q_i})}$$

一些性质：

- $D(p||q) \geq 0$ 分布相同时等于0，可以通过gibbs不等式证明
- $D(p||q) \neq D(q||p)$  不是距离指标
- $\nabla_{\theta}D(p||q_\theta)=\nabla_{\theta}H(p,q_\theta)-\nabla_{\theta}H(p)=\nabla_{\theta}H(p,q_\theta)$ 最小化KL散度等同于最小化交叉熵，这是损失函数使用交叉熵的由来 